{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample RL World\n",
    "\n",
    "In this notebook, we create a sample RL World. A simple RL World consists of the following:\n",
    "\n",
    "1. **Environment:** An environemnt is a model of the world that is external to the Agent and provides the Agent with the Observations and the Reward. The reward could be at each timestamp i.e. every episode or could be given at the end.\n",
    "2. **Agent:** An agent is somebody or something that interacts with the Environment\n",
    "\n",
    "The Agent interacts with the Environment using the following channels:\n",
    "1. **Actions:** An action is something the the Agent performs in the Environment. It could be a single action or a set of actions.\n",
    "2. **Reward:** A reaward is something that the Environment provides the Agent with for taking an Action in the Environment.\n",
    "3. **Observations:** An observation is something that the Environemnt provides the Agent with and it represents the states around the Agent in the Environment.\n",
    "\n",
    "So, let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's code the Environment.\n",
    "\n",
    "The Environment Class contains the following:\n",
    "\n",
    "1. **Constructor:** that contains a variable defining the total number of episodes for which the Agent interacts with the Environment.\n",
    "2. **get_observations:** is a function to return the Observations from the Environment.\n",
    "3. **get_actions:** is a function defining the Action Space i.e. the actions that an Agent can take in the Environment.\n",
    "4. **is_done:** is a function that returns True if the Agent has reached the last episode else False.\n",
    "5. **action:** is a function that defines the actions that the Agent takes in the Envrionment. This function discards the action in the current implementation and returns the reward which is a random value in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL World Dummy Environment\n",
    "class Environment:\n",
    "    # Initialize the Total Number of Episodes\n",
    "    def __init__(self):\n",
    "        self.num_episodes = 10\n",
    "    \n",
    "    # Function to get Observations Space from \n",
    "    # the Environment and Return to the Agent\n",
    "    def get_observations(self) -> List[float]:\n",
    "        return np.asarray([0.0, 0.0, 0.0])\n",
    "    \n",
    "    # Function to get Action Space\n",
    "    # This allows the Agent to query the set of Actions it can execute\n",
    "    def get_actions(self) -> List[int]:\n",
    "        return np.asarray([0, 1])\n",
    "    \n",
    "    # Function to check if we have reached the end of Episode\n",
    "    def is_done(self) -> bool:\n",
    "        return self.num_episodes == 0\n",
    "    \n",
    "    # Function to perform Action in the Environment\n",
    "    # This function handles Agent's Actions and returns the reward for this action\n",
    "    # Here, the Reward is random and its Action is discarded\n",
    "    def action(self, action: int) -> float:\n",
    "        # If game is completed, return\n",
    "        if self.is_done():\n",
    "            return Exception(\"Game Over !!\")\n",
    "        # Decrement through the episodes\n",
    "        self.num_episodes -= 1\n",
    "        # return Reward for random action taken\n",
    "        return np.random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's code the Agent.\n",
    "\n",
    "The Agent Class contains of the following:\n",
    "\n",
    "1. **Constructor:** that contains a variable defining the total reward.\n",
    "2. **step:** is a function that allows the Agent to step through the Environment, get the Reward and Observations to the Agent for the Actions and finally provides the total reward for all Ations performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Agent\n",
    "class Agent:\n",
    "    # total reward accumulated by the agent\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "    \n",
    "    # Function that allows an Agent to take a step in the Environment\n",
    "    # For every step, the Agent gets a set of Observations and a Reward\n",
    "    def step(self, env: Environment):\n",
    "        # Observe the Environment\n",
    "        current_obs = env.get_observations()\n",
    "        # Make a decision about the Action to take based on observations\n",
    "        action_space = env.get_actions()\n",
    "        # Submit the Action to the Environment and\n",
    "        # get the reward for the current step\n",
    "        actions = np.random.choice(action_space)\n",
    "        reward = env.action(actions)\n",
    "        # Add the reward\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        print(\"\\n------- Episode: {} --------\\n\".format(env.num_episodes))\n",
    "        print(\"Current Observation: {}\".format(current_obs))\n",
    "        print(\"Action Space: {}\".format(action_space))\n",
    "        print(\"Action: {}\".format(actions))\n",
    "        print(\"Reward: {}\".format(reward))\n",
    "        print(\"Total Reward: {}\".format(self.total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Instantiate the Environment and the Agent Classes and let the Agent step through it and finally print the total accumulated reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Episode: 9 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.46218286797379293\n",
      "Total Reward: 0.46218286797379293\n",
      "\n",
      "------- Episode: 8 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 0\n",
      "Reward: 0.8650581904636087\n",
      "Total Reward: 1.3272410584374015\n",
      "\n",
      "------- Episode: 7 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.14263078804708107\n",
      "Total Reward: 1.4698718464844824\n",
      "\n",
      "------- Episode: 6 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.8441566495841479\n",
      "Total Reward: 2.31402849606863\n",
      "\n",
      "------- Episode: 5 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.7524967797778965\n",
      "Total Reward: 3.0665252758465265\n",
      "\n",
      "------- Episode: 4 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.811125712262521\n",
      "Total Reward: 3.8776509881090475\n",
      "\n",
      "------- Episode: 3 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.8784343359296308\n",
      "Total Reward: 4.756085324038678\n",
      "\n",
      "------- Episode: 2 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 0\n",
      "Reward: 0.7084002872925412\n",
      "Total Reward: 5.464485611331219\n",
      "\n",
      "------- Episode: 1 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.04994596620962588\n",
      "Total Reward: 5.5144315775408455\n",
      "\n",
      "------- Episode: 0 --------\n",
      "\n",
      "Current Observation: [0. 0. 0.]\n",
      "Action Space: [0 1]\n",
      "Action: 1\n",
      "Reward: 0.4697935614624573\n",
      "Total Reward: 5.9842251390033026\n",
      "\n",
      "Total Reward at the End of 10 Episodes: 5.9842251390033026\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Environment Class\n",
    "env = Environment()\n",
    "\n",
    "# Instantiate the Agent Class\n",
    "agent = Agent()\n",
    "\n",
    "# Step thorugh the Environment till we reach the End of Episode\n",
    "while not env.is_done():\n",
    "    agent.step(env)\n",
    "print(\"\\nTotal Reward at the End of 10 Episodes: {}\".format(agent.total_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
